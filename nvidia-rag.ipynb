{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv langchain langchain-nvidia-ai-endpoints langchain-core pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing RAG Results with QuantexAI: Advanced Prompting Techniques for Developers\n",
    "\n",
    "In the rapidly advancing domain of AI, keeping up to date with the latest research and methodologies can be a daunting task for users and developers alike. A common challenge within AI systems is their tendency to \"hallucinate\" or generate inaccurate information when provided with insufficient context or data. This issue underscores the importance of innovative solutions to enhance the accuracy and reliability of AI-generated content.\n",
    "\n",
    "### Introduction to Retrieval Augmented Generation (RAG)\n",
    "\n",
    "To mitigate the problem of hallucinations in AI, the concept of Retrieval Augmented Generation (RAG) has been introduced. RAG addresses this issue by enabling AI models to query additional information relevant to the given question or prompt. This process enriches the AI's response with a broader context, potentially improving the accuracy and relevance of the generated output.\n",
    "\n",
    "However, the efficacy of RAG is contingent upon the quality and relevance of the information retrieved during the query process. A notable challenge arises when the required information is not successfully retrieved, often due to the imprecision in the query mechanism itself. This issue is particularly prevalent when users are unsure of the exact keywords or phrases stored within the Vector Database, which typically relies on distance-based or similarity search algorithms.\n",
    "\n",
    "### Advanced Prompting Techniques\n",
    "\n",
    "Recognizing these challenges, QuantexAI introduces a suite of advanced prompting techniques designed to significantly enhance the probability of retrieving the most relevant information through RAG. Our approach combines two innovative RAG techniques: MultiQuery Retrieval and Step-back Prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## Store your API key in a .env file in the root directory of this project, or set os.environ['NVIDIA_API_KEY'] to your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM we are using is the \"mixtral_8x7b\" model, which is one of the best open-source model available\n",
    "model = ChatNVIDIA(model=\"mixtral_8x7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiQuery Retrieval\n",
    "\n",
    "The process of data retrieval in AI systems, particularly in the context of RAG, is highly sensitive to the specific wording of queries. Subtle variations in query phrasing can lead to significantly different results, which poses a challenge in consistently retrieving the most relevant information.\n",
    "\n",
    "MultiQuery Retrieval extends the traditional single-query approach by simultaneously deploying multiple queries with variations in phrasing and context. This method increases the coverage of potential matches within the Vector Database, thereby increasing the likelihood of retrieving the most relevant information related to the user's request.\n",
    "\n",
    "### Automated Prompt Tuning with Language Models\n",
    "MultiQuery Retrieval leverages Large Language Models (LLMs) to automate the creation of varied queries based on the original prompt. This process involves generating multiple versions of the query from different perspectives, each designed to explore a unique angle or aspect of the information sought. By doing so, MultiQuery Retrieval broadens the scope of the search, enhancing the probability of matching with the relevant data stored within the Vector Database.\n",
    "\n",
    "### Diverse Perspectives and Keywords\n",
    "The key to MultiQuery Retrieval's effectiveness lies in its ability to use unique keywords and perspectives in the generated queries. This diversity ensures a wider coverage in the search process, increasing the chances of retrieving information that is most pertinent to the user's query. By systematically exploring a range of phrasings and contexts, MultiQuery Retrieval mitigates the limitations of conventional, single-query searches, which may miss critical information due to the constraints of specific wording.\n",
    "\n",
    "Source: https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the most recent advancements in extracting data from a vector database?',\n",
       " '2. How can one access the newest features for querying information within a vector database?',\n",
       " '3. In what ways have techniques for retrieving data evolved in vector databases recently?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model to represent a list of lines as output from the LLM.\n",
    "\n",
    "    Attributes:\n",
    "        lines (List[str]): A list of strings, each representing a line of text.\n",
    "    \"\"\"\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "# Custom output parser to convert LLM text output into a structured LineList\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    \"\"\"\n",
    "    An output parser that extends PydanticOutputParser to parse LLM output text into a structured LineList object.\n",
    "\n",
    "    Methods:\n",
    "        parse(text: str) -> LineList:\n",
    "            Parses a given text string into a LineList object by splitting the text into lines.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "# Initialize the output parser\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "# Define the prompt template for generating multi-query prompts\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"As an AI language model assistant, your primary objective is to enhance the user's ability to retrieve pertinent documents from a vector database through a distance-based \n",
    "                similarity search. To achieve this, create three distinct versions of the user's original question, focusing on using different wordings for each version. \n",
    "                Each version should offer a unique perspective or rephrasing, aiming to capture a broader range of potential search results by overcoming the inherent limitations of keyword-based searches in vector databases. Ensure that these alternative questions \n",
    "                are diverse in their phrasing and focus, yet remain true to the essence of the original query. Present the rephrased questions one after the other, separated by newlines.\n",
    "                Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Demonstrates the use of the multiquery_chain to process a question, generate multiple queries, and parse the output. Model is defined as mixtral_8x7b hosted by NVIDIA\n",
    "multiquery_chain = QUERY_PROMPT | model | LineListOutputParser()\n",
    "\n",
    "# Example usage\n",
    "res = multiquery_chain.invoke({\"question\":\"Latest development to query information from a vector database\"})\n",
    "\n",
    "res.lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-Back Prompting: Leveraging High-Level Concepts for Enhanced LLM Reasoning\n",
    "\n",
    "Step-back Prompting, a technique developed by Google Deepmind, involves strategically restructuring the prompt to include broader context or alternative formulations of the query. Before diving into the problem-solving process, the model is prompted to consider and articulate the underlying concepts and principles relevant to the query.\n",
    "\n",
    "### Chain-of-Thought (CoT) and Its Limitations\n",
    "While the Chain-of-Thought (CoT) prompting method has been recognized for improving AI model performance by outlining intermediate steps in reasoning, errors may occur within these intermediate steps, potentially leading to inaccurate conclusions. Step-back Prompting addresses this issue by reinforcing the model's foundational understanding before engaging in the step-by-step reasoning process.\n",
    "\n",
    "<p align=\"center\"><b>Step-back prompting technique</b><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*C-M88KonAuQhIGg5rlos-g.png\" alt=\"Step-back prompting technique\"></p>\n",
    "\n",
    "<p align=\"center\"><b>Comparison of performance among Base Models, CoT, and Step-back prompting</b><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*Jdv3FM1d625TJ7yDX4iHnQ.png\" alt=\"Comparison of performance among Base Models, CoT, and Step-back prompting\"></p>\n",
    "\n",
    "Source: https://python.langchain.com/docs/templates/stepback-qa-prompting, https://arxiv.org/pdf/2310.06117.pdf, https://cobusgreyling.medium.com/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are the latest developments in vector database technology?\\n\\n(This question is more generic and focuses on the overall technology of vector databases, rather than specifically on data extraction methods.)',\n",
       " 'what are the ways to obtain the most recent features for querying data in a vector database?',\n",
       " 'how have recent developments impacted data retrieval techniques in vector databases?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few-shot examples provided to guide the LLM in understanding the task\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transforming the examples into a format that can be used in chat-based prompting\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creating a few-shot prompt template with the provided examples\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Defining the main prompt template that incorporates few-shot examples to guide the LLM\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert at world knowledge. Your task is to step back \"\n",
    "            \"and paraphrase a question to one more generic step-back question, which \"\n",
    "            \"is easier to answer and gather foundational knowledge. Ensure only ONE generic step-back question is returned. ONLY RETURN step-back question.\"\n",
    "            \"Here are a few examples:\",\n",
    "        ),\n",
    "        # Integrating few-shot examples\n",
    "        few_shot_prompt,\n",
    "        # Placeholder for new question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuring the pipeline to generate step-back questions from the original queries\n",
    "question_gen = {\"question\":RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Generating step-back questions for each line in the previously obtained results\n",
    "stepback_answers = question_gen.batch(res.lines)\n",
    "stepback_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine and Summarize: Streamlining Information Presentation\n",
    "In the final stage of leveraging advanced prompting techniques with Retrieval Augmented Generation (RAG), the focus shifts towards the effective organization of information based on the information retrieved. This ensures that the vast array of data, whether sourced from a Vector Database, online searches, or other repositories, is coherently synthesized for the user.\n",
    "\n",
    "### The Refine Chain Process\n",
    "The refine chain process begins with generating an answer based on the first document retrieved. Subsequently, this initial answer undergoes a series of refinements with each additional document. This iterative approach allows for the gradual enhancement of the answer, incorporating broader perspectives and finer details with each step.\n",
    "\n",
    "Source: https://python.langchain.com/docs/use_cases/summarization#option-3.-refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the most recent advancements in extracting data from a vector database?',\n",
       " '2. How can one access the newest features for querying information within a vector database?',\n",
       " '3. In what ways have techniques for retrieving data evolved in vector databases recently?',\n",
       " 'what are the latest developments in vector database technology?\\n\\n(This question is more generic and focuses on the overall technology of vector databases, rather than specifically on data extraction methods.)',\n",
       " 'what are the ways to obtain the most recent features for querying data in a vector database?',\n",
       " 'how have recent developments impacted data retrieval techniques in vector databases?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.docstore.document import Document\n",
    "import time\n",
    "\n",
    "# Initialize the DuckDuckGo search functionality\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Combine the original queries and the generated step-back questions for searching\n",
    "queries_to_search = res.lines + stepback_answers\n",
    "\n",
    "queries_to_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The emergence of semantic search Sentence Embeddings: Enhancing search relevance Vector Database and Pinecone Step-1: Create a Pinecone Index Step-2: Loading Data into the index Step-3: Query the index Question answering and semantic search with GPT-4 Introduction The adoption of Vector Search and Vector Database Algorithms is witnessing a surge across various industries, revolutionizing how organizations leverage and interact with their data. E-commerce Personalization: Online retailers are leveraging Vector Search to enhance product recommendations. Vector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as... Source: Pinecone : The database indexes vectors using algorithms like PQ (Product Quantization), LSH (Locality-Sensitive Hashing), or HNSW (Hierarchical Navigable Small World). This step maps vectors to data structures for faster searching. Recently Vector databases have emerged as a groundbreaking approach to organize and search complex, high-dimensional data and when you combine this capability with Large Language Models (LLMs)...'),\n",
       " Document(page_content='To perform similarity search and retrieval in a vector database, you need to use a query vector that represents your desired information or criteria. The query vector can be either derived from the same type of data as the stored vectors (e.g., using an image as a query for an image database), or from different types of data (e.g., using text ... The vector embedding is inserted into the vector database, with some reference to the original content the embedding was created from. When the application issues a query, we use the same embedding model to create embeddings for the query and use those embeddings to query the database for similar vector embeddings. As mentioned before, those ... Vector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as... Even reorganizing the information within the prompt can make a big difference in how accurately LLMs understand the task. Researcher from the Stanford University has found that when important information is placed at the beginning or end of the prompt, the answer is usually more accurate. ... def query_vector_database(VECTOR_STORE_PATH ... Vector databases like Datastax Astra DB (built on Apache Cassandra) are designed to provide optimized storage and data access capabilities specifically for embeddings. A vector database is a type of database that is specifically designed to store and query high-dimensional vectors.'),\n",
       " Document(page_content='One of the primary techniques to augment LLMs with specific data and context and address hallucinations is retrieval augmented generation (i.e., RAG) and one of the critical components in a RAG architecture is a vector database. Vector search and retrieval employ the mathematical vectors to locate and retrieve data items, such as text, images, or sound, with shared characteristics. Large language models (LLMs) have allowed better encoding of the semantic relevance of text and images into these vector representations. · Follow 7 min read · Dec 2, 2023 -- Introduction In the ever-evolving landscape of artificial intelligence and machine learning, Retrieval-Augmented Generation (RAG) has emerged as a... We delved into various techniques like the bag-of-words, word2vec, and CNNs, gaining insights into how these methods transform raw data into meaningful vector representations. Our exploration extended to the crucial aspects of storing and indexing vector embeddings, and the significant contributions of vector libraries and databases in this realm. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature.'),\n",
       " Document(page_content='Generative AI + vector databases. Working to bridge the world of generative AI and vector databases into a new unified technology proposition, the KX team has this year taken its core kdb+ ... 1. Creation of Embeddings and Vectors. In the world of semantic search, embeddings are the cornerstone. They are high-dimensional vectors that represent data — be it text, images, or other types ... Searching trillions of vector datasets in milliseconds. Unstructured data management is simple. Reliable vector database that is always available. Highly scalable and adaptable. Search hybrid. Unified Lambda structure. Supported by the community and acknowledged by the industry. 3. Chroma. AI and Vector Databases: A Match Made in Data Heaven 🌌. The synergy between AI and vector databases is more crucial than ever. As we witness the rise of Large Language Models like GPT-3, the ability to efficiently manage and retrieve high-dimensional vectors becomes vital. Searching trillions of vector datasets in milliseconds. Simple unstructured data management. Highly scalable and adaptable. Search hybrid. Supported by a strong community. 3. Chroma. Website: Chroma | Open source: Yes | GitHub stars: 7k. Chroma DB is an open-source vector database tailored for AI-native embedding.'),\n",
       " Document(page_content='Step 4: Querying Data. To query the data, you just combine traditional SQL queries with vector operations. For instance, if you want to find products similar to a query vector, you can use the vector distance function. SELECT name, description, distance ( vector, query_vector) as dist FROM products ORDER BY dist LIMIT 5; Unlike traditional databases that rely on exact matches or predefined criteria for querying data, vector databases allow you to find the most similar or relevant data based on their... Vector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as... Indexing: The vector database indexes vectors using an algorithm such as PQ, LSH, or HNSW.This step maps the vectors to a data structure that will enable faster searching. Querying: The vector ... Bill McLane CTO Cloud What is a Vector Database? A vector database is a specialized storage system designed to efficiently handle and query high-dimensional vector data, commonly used in AI and machine learning applications for fast and accurate data retrieval.'),\n",
       " Document(page_content=\"As databases have developed over the (and let's just last half century) years, they have progressively extended and enhanced their capability to ingest, capture, snapshot, query, analyze and... Vector databases play a pivotal role in enabling data-driven decision-making processes by providing efficient storage, retrieval, and analysis capabilities for high-dimensional vector data. They ... Step 1 — Create embeddings or vectors using a model — Vectors can be created using models that are either free and open sourced or they can be created by calling API end points that are provided by... A vector database stores data as vector embeddings, which is a technique of translating data into numerical vectors that capture essential properties of the data. These vectors are stored in a high-dimensional space, where each dimension represents a unique feature of the data. This allows the database to perform low-latency retrieval of data ... Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search.batch(queries_to_search)\n",
    "\n",
    "# Due to DuckDuckGo's rate limitations, a batch search isn't feasible directly\n",
    "# Instead, perform searches one by one, with a delay to avoid rate limiting\n",
    "search_results = []\n",
    "for query in queries_to_search:\n",
    "    search_results.append(Document(page_content=search.run(query)))\n",
    "    time.sleep(5)\n",
    "    \n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Databases have significantly evolved over the past half-century, extending and enhancing their capabilities to ingest, capture, snapshot, query, analyze, and now even understand high-dimensional vector data. Vector databases play a crucial role in enabling data-driven decision-making processes by providing efficient storage, retrieval, and analysis capabilities for such data.\\n\\nThe process begins with creating embeddings or vectors using a model. These models can be free and open-sourced, or they can be created by calling API endpoints provided by various platforms. A vector database stores data as vector embeddings, which is a technique of translating data into numerical vectors that capture essential properties of the data. These vectors are stored in a high-dimensional space, where each dimension represents a unique feature of the data. This allows the database to perform low-latency retrieval of data, significantly improving data interaction across industries.\\n\\nThe adoption of vector search and vector databases is growing rapidly, with algorithms like PQ, LSH, or HNSW employed for efficient storage, indexing, and searching of high-dimensional data points. These databases are particularly beneficial for e-commerce personalization by enhancing product recommendations.\\n\\nRetrieval Augmented Generation (RAG), a primary technique to augment Large Language Models (LLMs) with specific data and context, often utilizes vector databases as a critical component. Vector search and retrieval employ mathematical vectors to locate and retrieve data items, such as text, images, or sound, with shared characteristics. Large language models have allowed better encoding of the semantic relevance of text and images into these vector representations.\\n\\nVector databases offer advanced applications when combined with Large Language Models. They allow for the efficient storage, indexing, and search of high-dimensional data points, often referred to as vector embedding databases. Querying data in vector databases involves combining traditional SQL queries with vector operations.\\n\\nResearch from Stanford University suggests that placing important information at the beginning or end of the prompt can improve the accuracy of LLMs in understanding tasks.\\n\\nKX team has been bridging the world of generative AI and vector databases with their core kdb+ technology. They have created Chroma, an open-source vector database tailored for AI-native embedding, handling high-dimensional vectors essential for managing and retrieving data efficiently, especially with the rise of Large Language Models like GPT-3. Chroma DB offers features such as searching trillions of vector datasets in milliseconds, simple unstructured data management, and high scalability and adaptability.\\n\\nIn summary, the use of vector databases and their efficient handling of high-dimensional data points are revolutionizing data interaction across industries, offering advanced applications when combined with Large Language Models. The rise of Retrieval Augmented Generation (RAG) has further emphasized the importance of vector databases as a critical component in augmenting language models with specific data and context. The KX team's Chroma, an open-source vector database, is specifically designed for AI-native embedding, making it a valuable tool in managing high-dimensional vectors for advanced applications.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Load the summarization chain designed for refining and summarizing information\n",
    "chain = load_summarize_chain(model, chain_type=\"refine\")\n",
    "\n",
    "# Invoke the summarization chain with the search results to obtain a refined summary\n",
    "res = chain.invoke(search_results)\n",
    "\n",
    "res['output_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
